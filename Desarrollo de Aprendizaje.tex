
\chapter{DESARROLLO DE APRENDIZAJE}

Capítulo compuesto por todo el trabajo de investigación sobre el trabajo realizado durante los meses de Noviembre hasta Enero inclusive.

Las siguientes secciones se basan en el estudio y comprensión de los artículos relacionados con los autoencoders. En dichas secciones, haré un breve resumen sobre lo entendido.

\section{Autocodificadores para la fusión de características no lineales: Taxonomía, modelos, software y directrices}

Los autoencoders (AEs) nos sirven para la fusión de características de forma no lineal.

Se recogerá una visión amplia de los AEs, su uso para la fusión de características, una taxonomía con diferentes modelos, cómo elegir el autoencoder adecuado para un problema determinado, diferentes softwares, y, por último, unos ejemplos de uso.

\begin{enumerate}
    \item \textbf{Introducción.} 
    \par Los autoencoders (AEs) son Redes Neuronales Artificiales (RNA) que codifican los datos de entrada y son entrenados para que al decodificar dichos datos, sean lo más parecido a las entradas.
    
    Es importante tener en cuenta que, aunque se ha desarrollado para el contexto del aprendizaje profundo, no todos los autoencoders son RNA con múltiples capas ocultas (profundo). Con esto sabemos que el AE puede ser RNA profunda o una RNA superficial con una única capa oculta.
    
    Está claro que debemos de evitar el problema de \textbf{\textit {The Curse of Dimensionality}}, el cual degrada un buen comportamiento de una RNA conforme aumenta el número de variables de entrada. Todo esto, nos lleva al auge en la Ingeniería de características.
    
    La selección del mejor subconjunto de variables de entrada es un problema combinatorio \textbf{NPhard problema combinatorio}. Además, sabemos que la evaluación de forma independiente de las variables nos lleva, dependiendo del problema, a perder información sobre los posibles pesos que existan en las relaciones entre ellas.
    
    A continuación, se obtiene una visión de los diferentes conceptos relacionados con las variables (características) de entrada:
    
    \begin{itemize}
        \item \textbf{Ingeniería de características:} Término global, caracterizado por su uso manual/automatizado de la selección/creación de características de entrada.
        
        \item \textbf{Aprendizaje de características:} Proceso automatizado de selección/creación de variables de entrada. Basado en los algoritmos que se verán posteriormente.
        
        \item \textbf{Aprendizaje de representación:} Basado en el aprendizaje de características, pero centrado en las RNA para automatizar por completo el proceso de \textbf{generación} de características (PLN, imágenes, etc). 
        
        \item \textbf{Selección de características:} Esencial en el preprocesamiento, basada en algoritmos que eligen las variables de entrada más informativas respecto a la/s variable/s de salida (supervisado) o evitando la redundancia entre ellas (no supervisado).
        
        \item \textbf{Extracción de características:} Técnicas como la normalización, la discretización, las transformaciones básicas aplicadas a determinados tipos de datos pertenencen a este término. Existe extracción mediante combinaciones lineales de las originales (PCA, no supervisados, o LDA, supervisados) y no lineales (algoritmos de \hyperlink{https://scikit-learn.org/stable/modules/manifold.html}{aprendizaje de pliegues}).
        
        \item \textbf{Fusión de características:} Pretenden combinar variables para eliminar información redundante e irrelevante a partir de datos multimodales. Varios AEs se enfocarán en dicho aspecto.
        
    \end{itemize}
    
    \newpage
    \item \textbf{Aspectos esenciales de los autoencoders.} \par Los AEs se entrenan para reconstruir su entrada en la capa de salida. 
    
    \begin{itemize}
        \item \textbf{Estructura general.}
            \par A partir de una entrada \textit{x}, se codifica y decodifica dicha entrada para obtener una reconstrucción \textit{r} (ambas deben tener la misma dimensionalidad). Sin embargo, \textit{y} (capa central) puede tener diferente dimensionalidad.
            
            En la figura ~\ref{AE_3capasE_3capasS}, tenemos un ejemplo de AE con una codificación de 3 capas y otras 3 capas de decodificación (capa central inclusive para ambos procesos).
            
            \begin{center}
                \includegraphics[width=.8\textwidth]{imagenes/AE_3capasE_3capasS.png} 
                \label{AE_3capasE_3capasS}
            \end{center}

        \item \textbf{Funciones de activación de uso común en los autocodificadores.}
            \par Una unidad (nodo) situada en cualquiera de las capas ocultas de la RNA obtiene una serie de valores entrantes de la capa anterior y calcula la suma ponderada de estas entradas, obteniendo la elección de la función a usar; es decir, la \textit{\textbf{función de activación}}. A este \textbf{comportamiento} se le considera en las RNA, normalmente, como \textbf{no lineal}.
            Las funciones de activación más comunes y simples son: lineal, binaria, ReLU, logística, tangente y SELU. 
            
            \smallskip
            Las unidades lineales rectificadas (ReLU) son populares en muchos modelos de aprendizaje profundo, pero es una función de activación que tiende a degradar el rendimiento del EA. (Si es negativo, entonces igual 0 y debilita la reconstrucción). Como opción a dicho problema, tenemos la \textit{\textbf{función SELU}}. Por último, tenemos las funciones sigmoides (logística y tangente) que son las más utilizadas.
            
            \begin{center}
                \includegraphics[width=.4\textwidth]{imagenes/Funciones_activacion_ReLU_SELU.png}
            \end{center}
            
        \item \textbf{Grupos de los AEs según la estructura de la red}.
        \par Hay muchas formas de clasificar los AEs, en este momento nos centramos en su estructura.
        
        Los AEs son RNA con una estructura simétrica; es decir, el codificador y decodificador tienen el mismo número de capas con el número de unidades (nodos) por capa con el número inverso [Figura ~\ref{AE_3capasE_3capasS}]. 
        
        Según la \textbf{dimesionalidad} (número de unidades) de la capa de codificación:
            \begin{enumerate}
                \item \textbf{Subcompletos}. Menor número de dimensionalidad (unidades/nodos) que de variables de entrada. Supone una representación compacta obligatoria al AE. Se hace mediante \textit{fusión de características originales} según sus pesos.
                
                \item \textbf{Excesivamente completos}. Dimensionalidad igual o mayor al número de características originales. Existe la posibilidad de replicar la función de identidad, el cual es un problema que debe ser restringido.
            \end{enumerate}
            
        Además, la estructura también depende del \textbf{número de capas}:
            \begin{enumerate}
                \item \textbf{Poco profundo}.Existe sólo una capa oculta (la de codificación).
                
                \item \textbf{Profundo}.Existen más de una capa oculta, el cual puede entrenarse capa a capa o como una RNA profunda.
            \end{enumerate}
        
        \newpage
        La Figura ~\ref{Estructuras_AEs_4tipos} nos muestra las diferentes estructuras explicadas. Rodeado en \color{red}rojo \color{black} está el mismo tipo de estructura según la \color{red}dimensionalidad \color{black} y rodeado en \color{blue}azul \color{black} está según el \color{blue}número de capas \color{black}. Ambos colores, agrupan al mismo tipo correspondiente.
            
        \begin{center} \includegraphics[width=.8\textwidth]{imagenes/Estructuras_AEs_4tipos.png}
        \label{Estructuras_AEs_4tipos}
        \end{center}
        
        \item \textbf{Taxonomía del autocodificador}.
        \par Para explicar la taxonomía, debemos de basarnos en la imagen que viene a continuación. En ella aparecen cuatro pilares:
            \begin{enumerate}
                \item \textbf{Baja dimensionalidad}. No realizan ningún tipo de aprendizaje de características de alto nivel y se ven obligados a optimizar una cantidad notable de parámetros. Esta tarea puede facilitarse simplemente reduciendo la dimensionalidad de los datos, y éste es el objetivo del \textbf{AE básico}.
                
                \item \textbf{Regularización}.A veces, es necesario requerir sobre las características aprendidas unas propiedades matemáticas específicas. 
                
                \item \textbf{Tolerancia al ruido}.Es deseable una alta robustez de los datos frente a datos ruidosos.
                
                \item \textbf{Modelo generativo}.En ocasiones, es útil mapear nuevas muestras en el espacio codificado sobre las características originales.
            
            \begin{center}  \includegraphics[width=.8\textwidth]{imagenes/AE_Taxonomia.png}
            \label{AE_Taxonomia}
            \end{center}
            
            \end{enumerate}
        
    \end{itemize}
    
    
    \item \textbf{Autocodificadores para la fusión de características.}
    
    \begin{enumerate}
        \item \textbf{Autoencoders básicos}. El objetivo principal de los AEs es el proceso de fusión de características en el que las características aprendidas presentan algunos rasgos deseados (menor dimensionalidad, mayor dispersión, etc). El modelo resultante puede generar nuevas instancias en el espacio de características latentes.
        \begin{enumerate}
            \item \textit{\textbf{Estructura}}. Las capas son de cantidad simétrica en unidades; es decir, las mismas de codificación y decodificación. Pero no necesitan ser simétricas respecto a las funciones de activación o la matriz de pesos.
            
            La más sencilla tiene una única capa oculta, definida por dos matrices de pesos y dos vectores de sesgos:
            \begin{center}
                $y = f(x) = s_{1} (W^{(1)}x + b^{(1)})$,
                
                $r = g (y) = s_{2} (W^{(2)}y + b^{(2)} )$
            \end{center}
            
            Siendo $s_{1}$ y $s_{2}$ las funciones de activación (normalmente, no lineales).
            
            La diferencia respecto a los AEs profundos es que llamaremos a la composición de funciones en el codificador \textit{f}, y la composición de funciones en el decodificador \textit{g}.
            
            \item \textit{\textbf{Función objetivo}}. Se basa en una pérdida por instancia (p.e., error cuadrático medio), por lo que si la función \textit{f} y la función \textit{g} son determinados por la matriz de pesos (\textit{W}) y los sesgos (\textit{b}), necesitamos optimizar \textit{W} y \textit{b} para minimizar la función objetivo.
            
            Por ejemplo, cuando los valores de entrada son
            binarios o modelados como bits, la entropía cruzada suele ser la alternativa preferida
            para la función de pérdida.
            \newpage
        
            \item \textit{\textbf{Entrenamiento}}. Los algoritmos más usados son los basados en la técnica del descenso de gradiente (\hyperlink{https://optimization.cbe.cornell.edu/index.php?title=Stochastic_gradient_descent}{SGD}, o variante de ello). En cada paso, el gradiente de la función objetivo con respecto a los parámetros muestra la dirección de mayor pendiente, y permite al algoritmo modificar los parámetros para buscar un mínimo de la función. Para el cálculo de los gradientes es necesario el \textbf{algoritmo de retropropagación}.
            
            Los AEs son susceptibles al \textbf{sobreajuste}, por lo que se puede aplicar regularización a la función objetivo consiguiendo mayor generalización tras fomentar pesos más pequeños que producen buenas reconstrucciones. De esta forma, los pesos \color{blue}señalados \color{black}  abarcan todos los pesos de W y el $\lambda$ \color{red}señalado \color{black} es un parámetro que determina la magnitud del decaimiento.
            
            \begin{center}
                \includegraphics[width=.8\textwidth]{imagenes/AE_basicos_funcion_objetivo_regularizacion.png}
                \label{AE_basicos_funcion_objetivo_regularizacion}
            \end{center}
            
            También existen otras posibilidades como, por ejemplo, provocar simetría entre las matrices de pesos y obtener una reducción de los parámetros consiguiendo eficiencia en los AEs sin alterar la arquitectura deseada.
            
            \item \textit{\textbf{Apilamiento}}. Para los AEs profundos, tenemos la posibilidad de entrenarlos capa por capa; es decir, \textbf{apilar} AEs básicos sucesivamente. Con esto obtenemos una mayor certeza de que los pesos iniciales para los AEs profundos es aceptable (el cuál es muy importante para que el proceso sea existoso).
            
            Aplicamos el entrenamiento capa por capa como se mostrará en la siguiente imagen, donde cada capa es entrenada sucesivamente hasta la codificación.
            \newpage
            
            \begin{center}
                \includegraphics[width=.8\textwidth]{imagenes/AE_apilamiento.png}
                \label{AE_apilamiento}
            \end{center}
            
        \end{enumerate}
        
        \item \textit{\textbf{Regularización}}. En AEs básicos, las codificaciones no presentan propiedades especiales. Cuando queremos que esto ocurra, añadimos a la función objetivo una penalización $\Omega$.
            
            
            \begin{enumerate}
                \item \textit{\textbf{Autoencoders dispersos}}. La escasez en una representación indica que los valores, en su mayoría, dan cero o cercanos a cero. Esto da motivo de uso a las transformaciones de muestras en códigos dispersos (sobrecompleto y significativo) en el aprendizaje automático. 
                
                En los AEs básicos, los códigos sobrecompletos serían entrenados para copiar sólamente la entradas en las salidas.
                
                Para añadir la dispersidad en un AE, se añade una penalización a la función de pérdida. Para comparar la activación deseada de una unidad (nodo), estas se pueden modelar con una  \textit{\textbf{variable aleatoria Bernoulli}}, donde sólo puede disparar o no.
                
                \begin{center}
                $\hat{p}_{i} = \frac{1}{|S|} \sum_{x \in S} f_{i}(x)$,
                \end{center}
                
                siendo $\hat{p}_{i}$ la media de la distribución Bernoulli asociada. Con esto, usaremos la \textit{\textbf{divergencia de Kullback-Leibler}}, siendo $p$ la activación media deseada, entre la variable aleatoria definida por la unidad \textit{i} y la correspondiente a las activaciones deseadas medirá la diferencia de distribución.
                
                \begin{center}
                $KL(p||\hat{p}_{i}) = p \log\frac{p}{\hat{p}_{i}} + (1 - p) \log\frac{1 - p}{1 - \hat{p}_{i}}$.
                \end{center}
                \newpage
                
                \begin{center}
                \includegraphics[width=.8\textwidth]{imagenes/AE_Regularizacion_dispersos_BernoulliKullback.png}
                \label{AE_Regularizacion_dispersos_BernoulliKullback}
                \end{center}
                
                Se observa que la penalización es baja cuando la activación media está cerca de la deseada, pero crece rápidamente a medida que se aleja.
                
                Por lo tanto, el término penalizante para la función objetivo es:
                
                \begin{center}
                $\Omega_{SAE}(W,b;S) = \sum_{i = 1}^{c} KL(p||\hat{p}_{i})$,
                \end{center}
                
                donde el valor de la media de activación $\hat{p}_{i}$ depende de lo parámetros del codificador y el conjunto de entrenamiento $S$.
                
                \item \textit{\textbf{Autoencoder contractivo}}. Logra invariabilidad a los cambios en muchas direcciones alrededor de las muestras. Para ello, usamos la \textit{\textbf{norma de Frobenius}} $|| \cdot ||_{F}$ de la matriz Jacobiana del codificador $J_{f}$ (cuanto mayor sea el valor, más inestable) y a partir de esta medida se construye la regularización en la función obtejtivo.
                
                $||J_{f}(x)||_{F}^{2} = \sum_{j = 1}^{d}\sum_{i = 1}^{c} (\frac{\partial f_{i}}{\partial x_{j} (x)})^2$ \hspace{5mm} $\Rightarrow$ \hspace{5mm} $\Omega_{SAE}(W,b;S) = \sum_{x \in S}|| J_{f}(x) ||_{F}^2$
                \newpage
            \end{enumerate}
            
            \item \textit{\textbf{Tolerancia al ruido}}
            \par Un AE puede aprender un espacio de características latente a partir de un conjunto de muestras, pero no garantiza la robustez ante la presencia de instancias ruidosas.
            
            \begin{enumerate}
                \item \textit{\textbf{Eliminación de ruido}}. Un DAE (Denoising AE) aprende a generar características robustas a partir de las entradas reconstruyendo las muestras parcialmente destruidas.
                
                Se aplica una técnica de corrupción estocástica en la fase de entrenamiento, basada en elegir aleatoriamente una cantidad fija de características para cada muestra de entrenamiento y ponerlas a 0 (u otros como el ruido gaussiano, el ruido de \textit{salt-and-pepper}). Los AEs se comparan con las originales y se entrenan para adivinar los valores perdidos.
                
                Un DAE no necesita más restricciones o regularizaciones para aprender una codificación significativa (es decir, puede ser sobrecompleto si se quiere).
                
                Además, se puede aplicar capa por capa para los AEs profundos. Para ello, las entradas no corrompidas se calculan como salidas de las capas anteriores. Entonces, estas entradas se corrompen y se proporcionan a la red.
                
                \begin{center}
                    \includegraphics[width = .4\textwidth]{imagenes/AE_ToleranciaRuido_EliminacionRuido.png}
                \end{center}
                
                \item \textit{\textbf{Autoencoder robusto}}. Basado en modificar la función de pérdida utilizada para minimizar el error de reconstrucción y neutralizar la sensibilidad a los distintos tipos de ruido.
                
                Se desarrolla a partir de una función de pérdida diferente, basada en la correntropía; es decir, medir específicamente la densidad de probabilidad de que dos eventos sean iguales. Se trata de minimizar dicho entropía negativa (maximizar) para obtener una mayor resistencia al ruido (sobretodo, el gaussiano).
                
            \end{enumerate}
            \newpage
            
            \item \textit{\textbf{Autoencoders específicos de dominio}}.
            \par Los siguientes AEs son de tipo estándar, pero específicos para modelar imágenes y secuencias.
            
            \begin{itemize}
                \item Autoencoder convolucional. AE estandar no considera la estructura bidimensional al procesar datos de imágenes. Por eso, los convolucionales lo resuelven usando capas totalmente conectadas. 
                
                \item Autoencoder LSTM. AE estandar no está preparado para modelar datos secuenciales, un AE LSTM lo consigue colocando unidades de memoria de largo plazo (LSTM) como codificador y decodificador de la red. El codificador
                LSTM lee y comprime una secuencia en una representación de tamaño fijo
                a partir de la cual el decodificador intenta extraer la secuencia original en
                orden inverso.
            \end{itemize}
            
        \item \textit{\textbf{Modelos generativos}}.
        \par Aprenden una distribución para poder extraer nuevas muestras (instancias), diferentes de las observadas. 
        
        \begin{itemize}
            \item Autoencoder variacional. Sustituyen las funciones deterministas en el codificador por mapeos estocásticos, calculando la función objetivo en virtud de las funciones de densidad de las variables aleatorias. 
            
            \begin{center}
                $\mathcal{L}_{VAE}(\theta,\phi;x) = KL(q_{\phi}(y|x)||p_{\theta}(y)) - \mathcal{E}_{q_{\phi}(y|x)}[\log p_{\theta}(x|y)]$,
            \end{center}
            donde $q$ es la distribución que aproxima la verdadera distribución latente de $y$, y $\theta$, $\phi$ son los parámetros de cada distribución.
            
            \item Autoencoder adversial. Modela la codificación imponiendo una distribución a priori, entrenando después un AE estándar y simultáneamente, una red discriminativa que intenta distinguir codificaciones a partir de muestras de la previa impuesta. Dado que el codificador se entrena para engañar al discriminador, las codificaciones tenderán a seguir la distribución impuesta.
        \end{itemize}
        
        \item \textit{\textbf{Otros AEs más allá de la fusión de características}}.
        \par Los siguientes AEs no entran en ninguna categoría anterior.
        
        \begin{itemize}
            \item Autoencoder relacional. Los AEs básicos no consideran explícitamente las relaciones entre instancias. Estos AEs relacionales modifican la función objetivo para tener en cuenta la fidelidad de la reconstrucción de las relaciones entre las muestras.
            \newpage
            
            \item Autoenconder discriminativo. Discrimina reuniendo las muestras positivas; es decir, realiza una mejor reconstrucción de la s instancias positivas que de las negativas. Para ello, se optimiza la función de pérdida (\textit{Hinge loss function}). Su objetivo se basa en la detección del objeto.
            
        \end{itemize} 
        
        \item \textit{\textbf{Arquitecturas basada en AEs para el aprendizaje de características}}.
        \par Los AEs básicos sirven como base para crear otras arquitecturas más complejas dedicadas a la \textbf{fusión de características}.
        
        \begin{itemize}
            \item Los árboles autoenconders. El codificador y decodificador son árboles de decisión, con \textbf{nodos de decisión suaves} (progagan sus instancias a todos los hijos con diferentes probabilidades).
            
            \item Autoencoder dual. Intenta aprender 2 representaciones latentes cuando en el problema las variables pueden ser tratadas como instancias y viceversa.
            
            \item Autoenconder relacional o correlación cruzada. Incorpora capas en las que las unidades se combinan por \textbf{multiplicación} en lugar de suma ponderada, ya que representa las correlaciones entre los componentes de sus entradas.
            
            \item Autoencoder recursivo. Árbol construido a partir de los AEs, donde se añade piezas de entrada (reconstruye) a medida que el modelo profundiza. Arquitectura usada para modelar el sentimiento en las frases.
        \end{itemize}

    \end{enumerate}
    
    \newpage\item \textit{\textbf{Comparación con otras técnicas de fusión de características}}.
    \par
    Los AEs son un tipo de técnica más dentro de las técnicas de fusión, las cuáles se clasifican por supervidados (uso de las distancias con información de la clase, como los autoencoders adversial) y \textbf{no supervisados} (nos centraremos en estos). 
    Diferenciamos entre \textbf{convexo} (optimiza funciones \textbf{sin ótpimos locales}) y \textbf{no convexa} (hay \textbf{óptimos locales}). Los \textbf{AEs} son \textbf{no convexos}. También diferenciamos entre enfoques lineales y no lineales.
    
    \begin{enumerate}
        \item \textbf{\textit{Enfoques lineales}}.
        \par
        Se busca el subconjunto de componentes de \textbf{mayor varianza ortogonal} entre ellas de forma que la combinación lineal se aplique a las variables iniciales.
        
        Los AEs pueden tratarse como un nivel general superior al PCA, diferenciados por el aprendizaje de combinaciones no lineales y representaciones incompletas de los datos.
        \vspace{5mm}
        \begin{center}
            \includegraphics[width = .8\textwidth]{imagenes/AE_enfoques_lineal_PCA.png}
        \end{center}
        \vspace{5mm}
        También tenemos el enfoque del \textbf{Análisis Factorial} (AF) que es diferenciado con PCA al igual que el AE básico y el varacional; es decir, los AF y AEs variacionales intentan encontrar el modelo que mejor describa las \textbf{variables subyacentes que causan los datos} mientras que los PCA y AEs básicos simplemente se basan en \textbf{la representación a baja dimensionalidad}.
        
        \newpage
        \item\textbf{\textbf{Enfoques no lineales}}.
        \par
        \textit{Kernel PCA} es una extensión de PCA que calcula combinaciones no lineales de variables y representaciones completas. Pero, \textbf{depende del kernel} el éxito y tiene un \textbf{comportamiento diferente} para cada problema.
        
        \vspace{5mm}
        El \textit{Escalamiento multidimensional} (MDS), basado en encontrar coordenadas en un espacio de menor dimensión, manteniendo las distancias relativas de los puntos.
        
        \textit{El mapeo de Sammon}. Modifica la función de coste clásica del MDS para dar mayor importancia a las distancias muy pequeñas respecto a las grandes.
        
        MDS para la \textbf{fusión de características no lineales} es \textbf{opuesto} al de los AEs, ya que los AEs no tienen en cuenta las distancias entre pares de muestras (optimizan una medida global de aptitud). 
        
        En resumen, podemos combinarla función objetivo del AE que considere las distancias (MDS) entre pares de puntos.
        
        \textit{Isomap}. Método ampliado de MDS, encargado de encontrar las \textbf{coordenadas} que describan los \textbf{grados de libertad reales} de los datos. 
        \textit{Locally Lineal Embedding} (LLE). Aprende a recoger los vecinos reconstruyendo linealmente cada punto a partir de sus vecinos para mantener la estructura local.
        
        Los \textbf{AEs} son capaces de \textbf{mapear nuevas instancias} de la superficie latente \textbf{después} de haber sido \textbf{entrenados}, mientras que los Isomap y LLE no están preparados.
        
        \textit{Laplacian Eigenmaps}. Construye un grafo de adyacencia (nodo por instancia, arista por conexión) donde se construye una matriz de pesos. Por último, obtiene los valores/vectores propios que son usados para calcular las coordenadas propias. Pero sabemos que los \textbf{AEs no} suelen tener en cuenta la \textbf{estructura local} de los datos (menos AEs contractivos y otros).
        
        \textit{Restricted Boltzman Machine} (RBM). Modelo gráfico no dirigido, definido por una distribución de probabilidad conjunta determinada por una función de energía. Los RBM son \textbf{alternativas a las AEs} para la \textbf{inicialización} codiciosa de los \textbf{pesos} en las RNAs. Pero los AEs son más adaptables a diferentes tareas que los RBM.
    
    \end{enumerate}
    
    \item \textbf{\textit{Aplicaciones en el aprendizaje de características y más allá}}.
    \par
    Los AEs ayudan a las siguientes tareas de la siguiente forma:
    \begin{itemize}
        \item \textbf{Clasifiación}. Reduce/Transforma los datos de entrenamiento para mejorar la optimización del resultado.
        
        \item\textbf{Compresión de datos}. Entrenan a tipos específicos de datos para aprender comprensiones eficientes.
        
        \item \textbf{Detección de patrones anormales}. Identifican las instancias discordantes mediante el análisis de las codificaciones generadas.
        
        \item \textbf{Hashing}. Conversión de los datos de entrada en un vector binario para su acceso eficiente.
        
        \item \textbf{Visualización}. Proyecciones gráficas de los datos en 2-3 dimensiones.
        
    \end{itemize}
    
    \item\textbf{\textit{Directtrices, software y ejemplos sobre el diseño de AEs}}.
    \par
    \begin{enumerate}
    \vspace{5mm}
        \item \textbf{\textit{Guidelines}}. 
        \par
        Cuando pensemos en construir un AE para un problema determinado, debemos pensar en las modificaciones de la \textbf{sección 3}, e incluso puede que elijamos combinaciones de ellos.
        
        En la siguiente imagen se muestra un resumen de los diferentes diseños de AEs.
        
        \vspace{5mm}
        \begin{center}
            \includegraphics[width = .8\textwidth]{imagenes/AE_disenios_p6.png}
        \end{center}
        \vspace{5mm}
        
        Las indicaciones básicas al diseño son las siguientes:
        
        \begin{itemize}
            \item \textit{Arquitectura}. Si tenemos un AE que tiene una longitud de codificación proporcionalmente muy baja respecto al número de variables originales, debemos de considerar un \textbf{AE apilado profundo}. Si fuese al contrario, entonces optar por \textbf{capas totalmente conectadas} .
            
            \item \textit{Activaciones y función de pérdida}. Las funciones de activación van relacionadas con la función de pérdida que se vaya a optimizar. Si tenemos una función de activación \textbf{sigmoide} suele ser razonable para la capa de codificación, siendo la última capa la preferida debido a sus mayores gradientes. \textbf{No} hace falta que éste \textbf{coincida} con la activación en la capa de \textbf{salida}. Mientras que usar \textbf{activación lineal o ReLU} es coherente si usamos el error cuadrático medio como error de reconstrucción. Por último, una \textbf{activación logística} se combina con el error de entropía cruzada y datos normalizados (por producir valores entre 0 y 1).
            
            \item \textit{Regularizaciones}. Lo principal es añadir un \textbf{decaimiento del peso} para evitar que se sobreajuste a los datos de entrenamiento. Una \textbf{codificación dispersa} es más flexible en la elección de la estructura. Además, una regularización por \textbf{contracción} puede ser valiosa si los datos forman una variedad de baja dimensión.
            
        \end{itemize}
        
        \vspace{5mm}
        \item \textbf{Softwares}.
        \par
        Entre los más comunes están: Tensorflow, Caffe, Torch, MXNet, Keras.
        
        El usuario debe definir el modelo capa por capa, colocando las activaciones donde se desee. Al indicar la función objetivo, serán las más habituales, pero al incluir las funciones de pérdida poco comunes (correntropía o regularizaciones como contracción) pueden necesitar ser implementadas adicionalmente.
        
        \vspace{5mm}
        \item \textbf{Ejemplo de uso con Tensorflow y Keras}.
        
        \par
        Se ha obtenido del repositorio de \hyperlink{https://github.com/fdavidcl/ae-review-resources}{fdavidcl} los scripts correspondientes al ejemplo de uso de autoencoders y PCA. 
        
        \begin{itemize}
            \item \textit{autoencoder.py}. Uso de AE con una activación indicada para la capa de codificación. 
            
            \item \textit{utils.py}. Implementa \textbf{regularizaciones} y \textbf{modificaciones} para la posibilidad de definir AEs básicos, dispersos, contractivos, de eliminación de ruido y robustos.
            
            \newpage
            \item \textit{mnist.py}. \textbf{Ejecutable} encargado de entrenar los AEs con los datos de MNIST y plasma una representación gráfica de la codificación y reconstrucción de algunas instancias de prueba.
            
            \item \textit{cancer.py}. \textbf{Ejecutable} que tiene como condición la necesidad del dataset de \textbf{WDBC} para su posible entrenamiento de un AE.
            
        \end{itemize}
        
        Debemos tener en cuenta que si queremos modificar el modelo de AE aprendido en uno de estos scripts, tendremos que modificar los parámetros que aparecen en la construcción del autoencoder.
        
        
    \end{enumerate}
    
    
\end{enumerate}

\newpage
\section{Análisis sobre el uso de autocodificadores para el aprendizaje de representaciones: Fundamentos, estudios de casos de tareas de aprendizaje, explicabilidad y desafíos.}

\begin{enumerate}
    \item \textbf{\textit{Introducción.}}
    \par
    Con las RNA profundas, el aprendizaje de representación se introdujo en los predictores; es decir, aprendizaje basado en optimizar iterativamente modificando los pesos en las capas de neuronas. Como las RNA pueden estructurarse conforme al problema, nos sirven también como aprendices de características independientes como los \textbf{Autoenconders} (arquitecturas neuronales que buscan la mejor representación de los datos según su función de pérdida indicada).
    
    En este trabajo se analizará una serie de resoluciones con una amplia variedad de autoencoders enfocados a diferentes aspectos. En mi caso, me centraré en el último caso, \textbf{la generación de instancias} que no pertenencen al conjunto de entrenamiento. 
    
    \vspace{5mm}
    \item\textbf{\textit{Generación de instancias.}}
    
    La representación aprendida por un AE puede ser útil para codificar o reconstruir instancias individuales a partir de un conjunto de entrenamiento. Hay que tener en cuenta que debe de tratar de realizar algún tipo de \textbf{aprendizaje múltiple}; mapeando el espacio de características en un espacio más pequeño dando sentido a trabajar con todo el espacio de codificación. A partir de este nuevo espacio reducido, se permite \textbf{predecir} una \textbf{reconstrucción} para codificaciones que \textbf{no} provienen de una \textbf{instancia} en el espacio de características original.
    
    Por ejemplo, generar nuevas imágenes de rostros similares a los de un conjunto de entrenamiento, pero \textbf{no idénticos} a ninguno de ellos. Hay que tener \textbf{cuidado} con la \textbf{interpolación}, ya que crearía muchas imágenes que no representan rostros. 
    
    Las variantes de AEs que pueden cumplir este propósito son: \textbf{AEs variacionales}, \textbf{adversariales} y \textbf{AEs contractivos}.
    
    \newpage
    
    \begin{itemize}
        \vspace{5mm}
        \item \textbf{AEs contractivos}. Se basa en el mapeo de las instancias a \textbf{codificaciones de los vecinos} más cercanos mediante una \textbf{regularización}. Por lo tanto, ayuda al AE a realizar transformaciones que encuentran colectores en los datos, ya que se preserva la estructura local. El colector puede entonces ser atravesado para que el decodificador genere nuevas instancias. 
        
        \vspace{5mm}
        \item \textbf{AEs variacionales y adversiales}. Crean una distribución a priori en las codificaciones de diferentes maneras, muestrean \textbf{nuevas instancias} tomando \textbf{puntos} de este \textbf{espacio} y proyectándolos \textbf{sobre el espacio de características original} en la \textbf{reconstrucción}. Los \textbf{variacionales} son \textbf{estacásticos}; es decir, para cada instancia no existe un único punto en el espacio de incrustación, sino que se les asigna una distribución (distribución normal, comúnmente usada mediante su media y desviación estandar).
        
        Se muestra a continuación un ejemplo de reconstrucción mediante el muestreo de esa distribución y la propagación de los resultados a través de la red de decodificadores.
        
        \vspace{5mm}
        \begin{center}
            \includegraphics[width = .6\textwidth]{imagenes/Ejemplos_AE_generativos_caras.png}
        \end{center}
        \vspace{5mm}
        
        \newpage
        En este ejemplo, se está usando un \textbf{AE variacional} basado en la estructura representada en la siguiente imagen, donde se generan rostros humanos que no pertenecen a ninguna persona, ya que no estarán presentes en el conjunto de datos de entrenamiento.
        
        \vspace{5mm}
        \begin{center}
            \includegraphics[width = .6\textwidth]{imagenes/AE_Variacional_Estructura_1.png}
        \end{center}
        \vspace{5mm}
    \end{itemize}
\end{enumerate}

\newpage


\section{Modelos Generativos Profundos.}




